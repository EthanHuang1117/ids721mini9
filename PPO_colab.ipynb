{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JowRQEGGKQ"
      },
      "source": [
        "################################################################################\n",
        "> # **Clone GitHub repository**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IyGzuEMQF6sJ",
        "outputId": "02497a14-b3e7-48a1-8c3e-21208a681c88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Cloning into 'PPO-PyTorch'...\n",
            "remote: Enumerating objects: 368, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 368 (delta 91), reused 68 (delta 68), pack-reused 264 (from 2)\u001b[K\n",
            "Receiving objects: 100% (368/368), 12.39 MiB | 12.60 MiB/s, done.\n",
            "Resolving deltas: 100% (150/150), done.\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "################# Clone repository from github to colab session ################\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to clone all the preTrained networks, logs, graph figures, gifs\n",
        "from the GitHub repository to this colab session\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "!git clone https://github.com/nikhilbarhate99/PPO-PyTorch\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mrn6rpJpF8Sc",
        "outputId": "cbbc1984-2661-4423-ae70-996f5bb1db2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "'./PPO-PyTorch/LICENSE' -> './LICENSE'\n",
            "'./PPO-PyTorch/make_gif.py' -> './make_gif.py'\n",
            "'./PPO-PyTorch/plot_graph.py' -> './plot_graph.py'\n",
            "'./PPO-PyTorch/PPO_colab.ipynb' -> './PPO_colab.ipynb'\n",
            "'./PPO-PyTorch/PPO_figs' -> './PPO_figs'\n",
            "'./PPO-PyTorch/PPO_figs/BipedalWalker-v2' -> './PPO_figs/BipedalWalker-v2'\n",
            "'./PPO-PyTorch/PPO_figs/BipedalWalker-v2/PPO_BipedalWalker-v2_fig_0.png' -> './PPO_figs/BipedalWalker-v2/PPO_BipedalWalker-v2_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/CartPole-v1' -> './PPO_figs/CartPole-v1'\n",
            "'./PPO-PyTorch/PPO_figs/CartPole-v1/PPO_CartPole-v1_fig_0.png' -> './PPO_figs/CartPole-v1/PPO_CartPole-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/LunarLander-v2' -> './PPO_figs/LunarLander-v2'\n",
            "'./PPO-PyTorch/PPO_figs/LunarLander-v2/PPO_LunarLander-v2_fig_0.png' -> './PPO_figs/LunarLander-v2/PPO_LunarLander-v2_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolHalfCheetah-v1' -> './PPO_figs/RoboschoolHalfCheetah-v1'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_fig_0.png' -> './PPO_figs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolHopper-v1' -> './PPO_figs/RoboschoolHopper-v1'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_fig_0.png' -> './PPO_figs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolWalker2d-v1' -> './PPO_figs/RoboschoolWalker2d-v1'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_0.png' -> './PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_1.png' -> './PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_1.png'\n",
            "'./PPO-PyTorch/PPO_gifs' -> './PPO_gifs'\n",
            "'./PPO-PyTorch/PPO_gifs/BipedalWalker-v2' -> './PPO_gifs/BipedalWalker-v2'\n",
            "'./PPO-PyTorch/PPO_gifs/BipedalWalker-v2/PPO_BipedalWalker-v2_gif_0.gif' -> './PPO_gifs/BipedalWalker-v2/PPO_BipedalWalker-v2_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/CartPole-v1' -> './PPO_gifs/CartPole-v1'\n",
            "'./PPO-PyTorch/PPO_gifs/CartPole-v1/PPO_CartPole-v1_gif_0.gif' -> './PPO_gifs/CartPole-v1/PPO_CartPole-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/LunarLander-v2' -> './PPO_gifs/LunarLander-v2'\n",
            "'./PPO-PyTorch/PPO_gifs/LunarLander-v2/PPO_LunarLander-v2_gif_0.gif' -> './PPO_gifs/LunarLander-v2/PPO_LunarLander-v2_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolHalfCheetah-v1' -> './PPO_gifs/RoboschoolHalfCheetah-v1'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_gif_0.gif' -> './PPO_gifs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolHopper-v1' -> './PPO_gifs/RoboschoolHopper-v1'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_gif_0.gif' -> './PPO_gifs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolWalker2d-v1' -> './PPO_gifs/RoboschoolWalker2d-v1'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_gif_0.gif' -> './PPO_gifs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_logs' -> './PPO_logs'\n",
            "'./PPO-PyTorch/PPO_logs/BipedalWalker-v2' -> './PPO_logs/BipedalWalker-v2'\n",
            "'./PPO-PyTorch/PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_0.csv' -> './PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_1.csv' -> './PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1' -> './PPO_logs/CartPole-v1'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1/PPO_CartPole-v1_log_0.csv' -> './PPO_logs/CartPole-v1/PPO_CartPole-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1/PPO_CartPole-v1_log_1.csv' -> './PPO_logs/CartPole-v1/PPO_CartPole-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1/PPO_CartPole-v1_log_2.csv' -> './PPO_logs/CartPole-v1/PPO_CartPole-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2' -> './PPO_logs/LunarLander-v2'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_0.csv' -> './PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_1.csv' -> './PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_2.csv' -> './PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1' -> './PPO_logs/RoboschoolHalfCheetah-v1'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_0.csv' -> './PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_1.csv' -> './PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_2.csv' -> './PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1' -> './PPO_logs/RoboschoolHopper-v1'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_0.csv' -> './PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_1.csv' -> './PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_2.csv' -> './PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1' -> './PPO_logs/RoboschoolWalker2d-v1'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_0.csv' -> './PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_1.csv' -> './PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_2.csv' -> './PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_preTrained' -> './PPO_preTrained'\n",
            "'./PPO-PyTorch/PPO_preTrained/BipedalWalker-v2' -> './PPO_preTrained/BipedalWalker-v2'\n",
            "'./PPO-PyTorch/PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth' -> './PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/CartPole-v1' -> './PPO_preTrained/CartPole-v1'\n",
            "'./PPO-PyTorch/PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth' -> './PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/LunarLander-v2' -> './PPO_preTrained/LunarLander-v2'\n",
            "'./PPO-PyTorch/PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth' -> './PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/README.md' -> './PPO_preTrained/README.md'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolHalfCheetah-v1' -> './PPO_preTrained/RoboschoolHalfCheetah-v1'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_0_0.pth' -> './PPO_preTrained/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolHopper-v1' -> './PPO_preTrained/RoboschoolHopper-v1'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_0_0.pth' -> './PPO_preTrained/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolWalker2d-v1' -> './PPO_preTrained/RoboschoolWalker2d-v1'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth' -> './PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO.py' -> './PPO.py'\n",
            "'./PPO-PyTorch/README.md' -> './README.md'\n",
            "'./PPO-PyTorch/test.py' -> './test.py'\n",
            "'./PPO-PyTorch/train.py' -> './train.py'\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to copy all files and folders from cloned folder (PPO-PyTorch)\n",
        "to current directory (/content/ or ./)\n",
        "\n",
        "So you can load preTrained networks and log files without changing any paths\n",
        "\n",
        "**  This will overwrite any saved networks, logs, graph figures, or gifs\n",
        "    that are created in this session before copying having the same name (or number)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "!cp -rv ./PPO-PyTorch/* ./\n",
        "\n",
        "print(\"============================================================================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-7AbGA2F8Ut",
        "outputId": "69609cf8-3ebf-434e-d0d6-9076022f69ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to delete original cloned folder and the cloned ipynb file\n",
        "(after you have copied its contents to current directory)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "# delete original cloned folder\n",
        "!rm -r ./PPO-PyTorch\n",
        "\n",
        "# delete cloned ipynb file\n",
        "!rm ./PPO_colab.ipynb\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4VJcUT2GlJz"
      },
      "source": [
        "################################################################################\n",
        "> # **Install Dependencies**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rbpSQTflGlAr",
        "outputId": "0588e85d-d48f-4c87-b4a8-c031a6780ab8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Collecting roboschool==1.0.7\n",
            "  Using cached roboschool-1.0.7.tar.gz (12.0 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym==0.15.4 in /usr/local/lib/python3.11/dist-packages (0.15.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (1.14.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (1.17.0)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (1.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (4.11.0.86)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (1.0.0)\n",
            "Building wheels for collected packages: roboschool\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for roboschool (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for roboschool\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for roboschool\n",
            "Failed to build roboschool\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (roboschool)\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pybullet in /usr/local/lib/python3.11/dist-packages (3.2.7)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: gym==0.15.4 in /usr/local/lib/python3.11/dist-packages (0.15.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (1.14.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (1.17.0)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (1.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (4.11.0.86)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (1.0.0)\n",
            "Collecting roboschool==1.0.7\n",
            "  Using cached roboschool-1.0.7.tar.gz (12.0 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (from roboschool==1.0.7) (0.15.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym->roboschool==1.0.7) (1.14.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym->roboschool==1.0.7) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from gym->roboschool==1.0.7) (1.17.0)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym->roboschool==1.0.7) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym->roboschool==1.0.7) (1.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from gym->roboschool==1.0.7) (4.11.0.86)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym->roboschool==1.0.7) (1.0.0)\n",
            "Building wheels for collected packages: roboschool\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for roboschool (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for roboschool\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for roboschool\n",
            "Failed to build roboschool\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (roboschool)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "############ install compatible version of OpenAI roboschool and gym ###########\n",
        "\n",
        "!pip install swig\n",
        "\n",
        "!pip install roboschool==1.0.7 gym==0.15.4\n",
        "\n",
        "# !pip install box2d-py\n",
        "\n",
        "# !pip install Box2D\n",
        "\n",
        "!pip install pybullet\n",
        "\n",
        "# !pip install gym[box2d]\n",
        "\n",
        "!pip install numpy==1.23.5\n",
        "!pip install gym==0.15.4\n",
        "!pip install roboschool==1.0.7\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzZairIiGQ11"
      },
      "source": [
        "################################################################################\n",
        "> # **Introduction**\n",
        "> The notebook is divided into 5 major parts :\n",
        "\n",
        "*   **Part I** : define actor-critic network and PPO algorithm\n",
        "*   **Part II** : train PPO algorithm and save network weights and log files\n",
        "*   **Part III** : load (preTrained) network weights and test PPO algorithm\n",
        "*   **Part IV** : load log files and plot graphs\n",
        "*   **Part V** : install xvbf, load (preTrained) network weights and save images for gif and then generate gif\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s37cJXAYGrTY"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - I**\n",
        "\n",
        "*   define actor critic networks\n",
        "*   define PPO algorithm\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT6VUBg-F8Zm",
        "outputId": "95fc8667-1f68-41de-a8dc-6d4b4061a403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Device set to : cpu\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "############################### Import libraries ###############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "# import roboschool\n",
        "import pybullet_envs\n",
        "\n",
        "\n",
        "################################## set device ##################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# set device to cpu or cuda\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################## PPO Policy ##################################\n",
        "\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.state_values = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.state_values[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_dim = action_dim\n",
        "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
        "\n",
        "        # actor\n",
        "        if has_continuous_action_space :\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Tanh()\n",
        "                        )\n",
        "        else:\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Softmax(dim=-1)\n",
        "                        )\n",
        "\n",
        "\n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 1)\n",
        "                    )\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        state_val = self.critic(state)\n",
        "\n",
        "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
        "\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            action_var = self.action_var.expand_as(action_mean)\n",
        "            cov_mat = torch.diag_embed(action_var).to(device)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "\n",
        "            # for single action continuous environments\n",
        "            if self.action_dim == 1:\n",
        "                action = action.reshape(-1, self.action_dim)\n",
        "\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "\n",
        "        return action_logprobs, state_values, dist_entropy\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_std = action_std_init\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = new_action_std\n",
        "            self.policy.set_action_std(new_action_std)\n",
        "            self.policy_old.set_action_std(new_action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = self.action_std - action_std_decay_rate\n",
        "            self.action_std = round(self.action_std, 4)\n",
        "            if (self.action_std <= min_action_std):\n",
        "                self.action_std = min_action_std\n",
        "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
        "            else:\n",
        "                print(\"setting actor output action_std to : \", self.action_std)\n",
        "            self.set_action_std(self.action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
        "\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "            self.buffer.state_values.append(state_val)\n",
        "\n",
        "            return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "            self.buffer.state_values.append(state_val)\n",
        "\n",
        "            return action.item()\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        # Monte Carlo estimate of returns\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
        "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
        "\n",
        "        # calculate advantages\n",
        "        advantages = rewards.detach() - old_state_values.detach()\n",
        "\n",
        "\n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            # Evaluating old actions and values\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "\n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # final loss of clipped objective PPO\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
        "\n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # clear buffer\n",
        "        self.buffer.clear()\n",
        "\n",
        "\n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "\n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-xCb_EyxF8cF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part I ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr-ZjT_CGyEi"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - II**\n",
        "\n",
        "*   train PPO algorithm on environments\n",
        "*   save preTrained networks weights and log files\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY1-DzVCF8eh",
        "outputId": "02f3d19b-d674-491c-be34-18b868e06750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "training environment name : CartPole-v1\n",
            "current logging run number for CartPole-v1 :  5\n",
            "logging at : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_5.csv\n",
            "save checkpoint path : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "max training timesteps :  100000\n",
            "max timesteps per episode :  400\n",
            "model saving frequency : 20000 timesteps\n",
            "log frequency : 800 timesteps\n",
            "printing average reward over episodes in last : 1600 timesteps\n",
            "--------------------------------------------------------------------------------------------\n",
            "state space dimension :  4\n",
            "action space dimension :  2\n",
            "--------------------------------------------------------------------------------------------\n",
            "Initializing a discrete action space policy\n",
            "--------------------------------------------------------------------------------------------\n",
            "PPO update frequency : 1600 timesteps\n",
            "PPO K epochs :  40\n",
            "PPO epsilon clip :  0.2\n",
            "discount factor (gamma) :  0.99\n",
            "--------------------------------------------------------------------------------------------\n",
            "optimizer learning rate actor :  0.0003\n",
            "optimizer learning rate critic :  0.001\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2025-04-04 20:15:07\n",
            "============================================================================================\n",
            "Episode : 82 \t\t Timestep : 1600 \t\t Average Reward : 19.43\n",
            "Episode : 149 \t\t Timestep : 3200 \t\t Average Reward : 23.79\n",
            "Episode : 204 \t\t Timestep : 4800 \t\t Average Reward : 29.07\n",
            "Episode : 236 \t\t Timestep : 6400 \t\t Average Reward : 49.59\n",
            "Episode : 256 \t\t Timestep : 8000 \t\t Average Reward : 81.25\n",
            "Episode : 276 \t\t Timestep : 9600 \t\t Average Reward : 79.0\n",
            "Episode : 292 \t\t Timestep : 11200 \t\t Average Reward : 98.44\n",
            "Episode : 306 \t\t Timestep : 12800 \t\t Average Reward : 112.93\n",
            "Episode : 314 \t\t Timestep : 14400 \t\t Average Reward : 196.25\n",
            "Episode : 325 \t\t Timestep : 16000 \t\t Average Reward : 146.55\n",
            "Episode : 332 \t\t Timestep : 17600 \t\t Average Reward : 225.71\n",
            "Episode : 339 \t\t Timestep : 19200 \t\t Average Reward : 239.57\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:27\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 345 \t\t Timestep : 20800 \t\t Average Reward : 254.17\n",
            "Episode : 350 \t\t Timestep : 22400 \t\t Average Reward : 304.0\n",
            "Episode : 356 \t\t Timestep : 24000 \t\t Average Reward : 272.83\n",
            "Episode : 362 \t\t Timestep : 25600 \t\t Average Reward : 287.5\n",
            "Episode : 367 \t\t Timestep : 27200 \t\t Average Reward : 302.4\n",
            "Episode : 372 \t\t Timestep : 28800 \t\t Average Reward : 291.0\n",
            "Episode : 378 \t\t Timestep : 30400 \t\t Average Reward : 302.17\n",
            "Episode : 382 \t\t Timestep : 32000 \t\t Average Reward : 366.5\n",
            "Episode : 387 \t\t Timestep : 33600 \t\t Average Reward : 331.2\n",
            "Episode : 392 \t\t Timestep : 35200 \t\t Average Reward : 329.2\n",
            "Episode : 397 \t\t Timestep : 36800 \t\t Average Reward : 325.2\n",
            "Episode : 403 \t\t Timestep : 38400 \t\t Average Reward : 254.83\n",
            "Episode : 408 \t\t Timestep : 40000 \t\t Average Reward : 309.6\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:47\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 413 \t\t Timestep : 41600 \t\t Average Reward : 311.6\n",
            "Episode : 417 \t\t Timestep : 43200 \t\t Average Reward : 378.0\n",
            "Episode : 421 \t\t Timestep : 44800 \t\t Average Reward : 400.0\n",
            "Episode : 425 \t\t Timestep : 46400 \t\t Average Reward : 400.0\n",
            "Episode : 429 \t\t Timestep : 48000 \t\t Average Reward : 400.0\n",
            "Episode : 434 \t\t Timestep : 49600 \t\t Average Reward : 332.2\n",
            "Episode : 440 \t\t Timestep : 51200 \t\t Average Reward : 286.0\n",
            "Episode : 445 \t\t Timestep : 52800 \t\t Average Reward : 300.2\n",
            "Episode : 449 \t\t Timestep : 54400 \t\t Average Reward : 358.25\n",
            "Episode : 454 \t\t Timestep : 56000 \t\t Average Reward : 394.2\n",
            "Episode : 458 \t\t Timestep : 57600 \t\t Average Reward : 400.0\n",
            "Episode : 463 \t\t Timestep : 59200 \t\t Average Reward : 261.0\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:08\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 469 \t\t Timestep : 60800 \t\t Average Reward : 288.5\n",
            "Episode : 475 \t\t Timestep : 62400 \t\t Average Reward : 276.33\n",
            "Episode : 479 \t\t Timestep : 64000 \t\t Average Reward : 395.0\n",
            "Episode : 483 \t\t Timestep : 65600 \t\t Average Reward : 400.0\n",
            "Episode : 487 \t\t Timestep : 67200 \t\t Average Reward : 400.0\n",
            "Episode : 491 \t\t Timestep : 68800 \t\t Average Reward : 400.0\n",
            "Episode : 495 \t\t Timestep : 70400 \t\t Average Reward : 400.0\n",
            "Episode : 499 \t\t Timestep : 72000 \t\t Average Reward : 400.0\n",
            "Episode : 503 \t\t Timestep : 73600 \t\t Average Reward : 400.0\n",
            "Episode : 507 \t\t Timestep : 75200 \t\t Average Reward : 400.0\n",
            "Episode : 511 \t\t Timestep : 76800 \t\t Average Reward : 348.75\n",
            "Episode : 516 \t\t Timestep : 78400 \t\t Average Reward : 376.0\n",
            "Episode : 520 \t\t Timestep : 80000 \t\t Average Reward : 400.0\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:27\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 524 \t\t Timestep : 81600 \t\t Average Reward : 400.0\n",
            "Episode : 528 \t\t Timestep : 83200 \t\t Average Reward : 400.0\n",
            "Episode : 532 \t\t Timestep : 84800 \t\t Average Reward : 368.25\n",
            "Episode : 536 \t\t Timestep : 86400 \t\t Average Reward : 400.0\n",
            "Episode : 540 \t\t Timestep : 88000 \t\t Average Reward : 400.0\n",
            "Episode : 544 \t\t Timestep : 89600 \t\t Average Reward : 400.0\n",
            "Episode : 548 \t\t Timestep : 91200 \t\t Average Reward : 382.75\n",
            "Episode : 552 \t\t Timestep : 92800 \t\t Average Reward : 391.0\n",
            "Episode : 557 \t\t Timestep : 94400 \t\t Average Reward : 336.0\n",
            "Episode : 561 \t\t Timestep : 96000 \t\t Average Reward : 400.0\n",
            "Episode : 565 \t\t Timestep : 97600 \t\t Average Reward : 400.0\n",
            "Episode : 569 \t\t Timestep : 99200 \t\t Average Reward : 400.0\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:49\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2025-04-04 20:15:07\n",
            "Finished training at (GMT) :  2025-04-04 20:16:56\n",
            "Total training time  :  0:01:49\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################################### Training ###################################\n",
        "\n",
        "\n",
        "####### initialize environment hyperparameters ######\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "\n",
        "max_ep_len = 400                    # max timesteps in one episode\n",
        "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
        "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
        "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
        "\n",
        "action_std = None\n",
        "\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "## Note : print/log frequencies should be > than max_ep_len\n",
        "\n",
        "\n",
        "################ PPO hyperparameters ################\n",
        "\n",
        "\n",
        "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
        "K_epochs = 40               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003       # learning rate for actor network\n",
        "lr_critic = 0.001       # learning rate for critic network\n",
        "\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "\n",
        "print(\"training environment name : \" + env_name)\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "###################### logging ######################\n",
        "\n",
        "#### log files for multiple runs are NOT overwritten\n",
        "\n",
        "log_dir = \"PPO_logs\"\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "log_dir = log_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "#### get number of log files in log directory\n",
        "run_num = 0\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "run_num = len(current_num_files)\n",
        "\n",
        "\n",
        "#### create new log file for each run\n",
        "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "\n",
        "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
        "print(\"logging at : \" + log_f_name)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "################### checkpointing ###################\n",
        "\n",
        "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
        "\n",
        "directory = \"PPO_preTrained\"\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "directory = directory + '/' + env_name + '/'\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"save checkpoint path : \" + checkpoint_path)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "############# print all hyperparameters #############\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"max training timesteps : \", max_training_timesteps)\n",
        "print(\"max timesteps per episode : \", max_ep_len)\n",
        "\n",
        "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
        "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
        "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"state space dimension : \", state_dim)\n",
        "print(\"action space dimension : \", action_dim)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "if has_continuous_action_space:\n",
        "    print(\"Initializing a continuous action space policy\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"starting std of action distribution : \", action_std)\n",
        "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
        "    print(\"minimum std of action distribution : \", min_action_std)\n",
        "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
        "\n",
        "else:\n",
        "    print(\"Initializing a discrete action space policy\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
        "print(\"PPO K epochs : \", K_epochs)\n",
        "print(\"PPO epsilon clip : \", eps_clip)\n",
        "print(\"discount factor (gamma) : \", gamma)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"optimizer learning rate actor : \", lr_actor)\n",
        "print(\"optimizer learning rate critic : \", lr_critic)\n",
        "\n",
        "if random_seed:\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"setting random seed to \", random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "################# training procedure ################\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# track total training time\n",
        "start_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# logging file\n",
        "log_f = open(log_f_name,\"w+\")\n",
        "log_f.write('episode,timestep,reward\\n')\n",
        "\n",
        "\n",
        "# printing and logging variables\n",
        "print_running_reward = 0\n",
        "print_running_episodes = 0\n",
        "\n",
        "log_running_reward = 0\n",
        "log_running_episodes = 0\n",
        "\n",
        "time_step = 0\n",
        "i_episode = 0\n",
        "\n",
        "\n",
        "# training loop\n",
        "while time_step <= max_training_timesteps:\n",
        "\n",
        "    state = env.reset()\n",
        "    current_ep_reward = 0\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "\n",
        "        # select action with policy\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # saving reward and is_terminals\n",
        "        ppo_agent.buffer.rewards.append(reward)\n",
        "        ppo_agent.buffer.is_terminals.append(done)\n",
        "\n",
        "        time_step +=1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        # update PPO agent\n",
        "        if time_step % update_timestep == 0:\n",
        "            ppo_agent.update()\n",
        "\n",
        "        # if continuous action space; then decay action std of ouput action distribution\n",
        "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
        "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
        "\n",
        "        # log in logging file\n",
        "        if time_step % log_freq == 0:\n",
        "\n",
        "            # log average reward till last episode\n",
        "            log_avg_reward = log_running_reward / log_running_episodes\n",
        "            log_avg_reward = round(log_avg_reward, 4)\n",
        "\n",
        "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
        "            log_f.flush()\n",
        "\n",
        "            log_running_reward = 0\n",
        "            log_running_episodes = 0\n",
        "\n",
        "        # printing average reward\n",
        "        if time_step % print_freq == 0:\n",
        "\n",
        "            # print average reward till last episode\n",
        "            print_avg_reward = print_running_reward / print_running_episodes\n",
        "            print_avg_reward = round(print_avg_reward, 2)\n",
        "\n",
        "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
        "\n",
        "            print_running_reward = 0\n",
        "            print_running_episodes = 0\n",
        "\n",
        "        # save model weights\n",
        "        if time_step % save_model_freq == 0:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"saving model at : \" + checkpoint_path)\n",
        "            ppo_agent.save(checkpoint_path)\n",
        "            print(\"model saved\")\n",
        "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        # break; if the episode is over\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print_running_reward += current_ep_reward\n",
        "    print_running_episodes += 1\n",
        "\n",
        "    log_running_reward += current_ep_reward\n",
        "    log_running_episodes += 1\n",
        "\n",
        "    i_episode += 1\n",
        "\n",
        "\n",
        "log_f.close()\n",
        "env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print total training time\n",
        "print(\"============================================================================================\")\n",
        "end_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "print(\"Finished training at (GMT) : \", end_time)\n",
        "print(\"Total training time  : \", end_time - start_time)\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UEy2qKdZF8ha"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part II ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhK13_1G6zX"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - III**\n",
        "\n",
        "*   load and test preTrained networks on environments\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZWyhkq9Gxm5",
        "outputId": "9223570a-338b-490c-90ac-081074b40d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading network from : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode: 1 \t\t Reward: 400.0\n",
            "Episode: 2 \t\t Reward: 400.0\n",
            "Episode: 3 \t\t Reward: 400.0\n",
            "Episode: 4 \t\t Reward: 400.0\n",
            "Episode: 5 \t\t Reward: 400.0\n",
            "Episode: 6 \t\t Reward: 400.0\n",
            "Episode: 7 \t\t Reward: 400.0\n",
            "Episode: 8 \t\t Reward: 400.0\n",
            "Episode: 9 \t\t Reward: 400.0\n",
            "Episode: 10 \t\t Reward: 400.0\n",
            "============================================================================================\n",
            "average test reward : 400.0\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "#################################### Testing ###################################\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "max_ep_len = 400\n",
        "action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"LunarLander-v2\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 300\n",
        "# action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "# env_name = \"RoboschoolWalker2d-v1\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 10    # total num of testing episodes\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003           # learning rate for actor\n",
        "lr_critic = 0.001           # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # clear buffer\n",
        "    ppo_agent.buffer.clear()\n",
        "\n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "n6IYC_JCGxlB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part III ###############################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZewQELovHFt4"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - IV**\n",
        "\n",
        "*   load log files using pandas\n",
        "*   plot graph using matplotlib\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bY-E5HGcGxiu",
        "outputId": "efebc5a0-bf36-4855-9bae-ef667a00b212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_0.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_1.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_2.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_3.csv\n",
            "data shape :  (0, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_4.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_5.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-879859696293>:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_concat = pd.concat(all_runs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "figure saved at :  PPO_figs/CartPole-v1//PPO_CartPole-v1_fig_0.png\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "fig_num = 0     #### change this to prevent overwriting figures in same env_name folder\n",
        "\n",
        "plot_avg = True    # plot average of all runs; else plot all runs separately\n",
        "\n",
        "fig_width = 10\n",
        "fig_height = 6\n",
        "\n",
        "\n",
        "# smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "window_len_smooth = 50\n",
        "min_window_len_smooth = 1\n",
        "linewidth_smooth = 1.5\n",
        "alpha_smooth = 1\n",
        "\n",
        "window_len_var = 5\n",
        "min_window_len_var = 1\n",
        "linewidth_var = 2\n",
        "alpha_var = 0.1\n",
        "\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'olive', 'brown', 'magenta', 'cyan', 'crimson','gray', 'black']\n",
        "\n",
        "\n",
        "# make directory for saving figures\n",
        "figures_dir = \"PPO_figs\"\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "# make environment directory for saving figures\n",
        "figures_dir = figures_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "\n",
        "fig_save_path = figures_dir + '/PPO_' + env_name + '_fig_' + str(fig_num) + '.png'\n",
        "\n",
        "\n",
        "# get number of log files in directory\n",
        "log_dir = \"PPO_logs\" + '/' + env_name + '/'\n",
        "\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "num_runs = len(current_num_files)\n",
        "\n",
        "\n",
        "all_runs = []\n",
        "\n",
        "for run_num in range(num_runs):\n",
        "\n",
        "    log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "    print(\"loading data from : \" + log_f_name)\n",
        "    data = pd.read_csv(log_f_name)\n",
        "    data = pd.DataFrame(data)\n",
        "\n",
        "    print(\"data shape : \", data.shape)\n",
        "\n",
        "    all_runs.append(data)\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "if plot_avg:\n",
        "    # average all runs\n",
        "    df_concat = pd.concat(all_runs)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "    data_avg['reward_smooth'] = data_avg['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "    data_avg['reward_var'] = data_avg['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_smooth',ax=ax,color=colors[0],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_var',ax=ax,color=colors[0],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep only reward_smooth in the legend and rename it\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
        "\n",
        "\n",
        "else:\n",
        "    for i, run in enumerate(all_runs):\n",
        "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "        run['reward_smooth_' + str(i)] = run['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "        run['reward_var_' + str(i)] = run['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "        # plot the lines\n",
        "        run.plot(kind='line', x='timestep' , y='reward_smooth_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "        run.plot(kind='line', x='timestep' , y='reward_var_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep alternate elements (reward_smooth_i) in the legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_handles = []\n",
        "    new_labels = []\n",
        "    for i in range(len(handles)):\n",
        "        if(i%2 == 0):\n",
        "            new_handles.append(handles[i])\n",
        "            new_labels.append(labels[i])\n",
        "    ax.legend(new_handles, new_labels, loc=2)\n",
        "\n",
        "\n",
        "\n",
        "# ax.set_yticks(np.arange(0, 1800, 200))\n",
        "# ax.set_xticks(np.arange(0, int(4e6), int(5e5)))\n",
        "\n",
        "\n",
        "ax.grid(color='gray', linestyle='-', linewidth=1, alpha=0.2)\n",
        "\n",
        "ax.set_xlabel(\"Timesteps\", fontsize=12)\n",
        "ax.set_ylabel(\"Rewards\", fontsize=12)\n",
        "\n",
        "plt.title(env_name, fontsize=14)\n",
        "\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(fig_width, fig_height)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "plt.savefig(fig_save_path)\n",
        "print(\"figure saved at : \", fig_save_path)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YaWPRW9EGxgH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part IV ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8uG43MtHNGC"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - V**\n",
        "\n",
        "*   install virtual display libraries for rendering on colab / remote server ^\n",
        "*   load preTrained networks and save images for gif\n",
        "*   generate and save gif from previously saved images\n",
        "\n",
        "*   ^ If running locally; do not install xvbf and pyvirtualdisplay. Just comment out the virtual display code and render it normally.\n",
        "*   ^ You will still require to use ipythondisplay, if you want to render it in the Jupyter Notebook.\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VL3tpKf3HLAq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#### to render on colab / server / headless machine install virtual display libraries\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pybullet_envs\n",
        "import gym"
      ],
      "metadata": {
        "id": "d0jfOrdhbz4E"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "j5Rx_IFKHK-D",
        "outputId": "0a7fc5ed-eea5-4727-b097-93d1d6247dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'roboschool'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3c6e16788d01>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mroboschool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'roboschool'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "\n",
        "############################# save images for gif ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import gym\n",
        "import roboschool\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "One frame corresponding to each timestep is saved in a folder :\n",
        "\n",
        "PPO_gif_images/env_name/000001.jpg\n",
        "PPO_gif_images/env_name/000002.jpg\n",
        "PPO_gif_images/env_name/000003.jpg\n",
        "...\n",
        "...\n",
        "...\n",
        "\n",
        "\n",
        "if this section is run multiple times or for multiple episodes for the same env_name;\n",
        "then the saved images will be overwritten.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### beginning of virtual display code section\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "#### end of virtual display code section\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "max_ep_len = 400\n",
        "action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"LunarLander-v2\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 300\n",
        "# action_std = None\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "# env_name = \"RoboschoolWalker2d-v1\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 1     # save gif for only one episode\n",
        "\n",
        "render_ipython = False      # plot the images using matplotlib and ipythondisplay before saving (slow)\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003         # learning rate for actor\n",
        "lr_critic = 0.001         # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "# make directory for saving gif images\n",
        "gif_images_dir = \"PPO_gif_images\" + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make environment directory for saving gif images\n",
        "gif_images_dir = gif_images_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make directory for gif\n",
        "gif_dir = \"PPO_gifs\" + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "# make environment directory for gif\n",
        "gif_dir = gif_dir + '/' + env_name  + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "\n",
        "\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        img = env.render(mode = 'rgb_array')\n",
        "\n",
        "\n",
        "        #### beginning of ipythondisplay code section 1\n",
        "\n",
        "        if render_ipython:\n",
        "            plt.imshow(img)\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "\n",
        "        #### end of ipythondisplay code section 1\n",
        "\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(gif_images_dir + '/' + str(t).zfill(6) + '.jpg')\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # clear buffer\n",
        "    ppo_agent.buffer.clear()\n",
        "\n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "#### beginning of ipythondisplay code section 2\n",
        "\n",
        "if render_ipython:\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "\n",
        "#### end of ipythondisplay code section 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "print(\"total number of frames / timesteps / images saved : \", t)\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoVshl_ZHK7s"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "######################## generate gif from saved images ########################\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "gif_num = 0     #### change this to prevent overwriting gifs in same env_name folder\n",
        "\n",
        "# adjust following parameters to get desired duration, size (bytes) and smoothness of gif\n",
        "total_timesteps = 300\n",
        "step = 10\n",
        "frame_duration = 150\n",
        "\n",
        "\n",
        "# input images\n",
        "gif_images_dir = \"PPO_gif_images/\" + env_name + '/*.jpg'\n",
        "\n",
        "\n",
        "# ouput gif path\n",
        "gif_dir = \"PPO_gifs\"\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_dir = gif_dir + '/' + env_name\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_path = gif_dir + '/PPO_' + env_name + '_gif_' + str(gif_num) + \"ECE590' + .gif'\n",
        "\n",
        "\n",
        "\n",
        "img_paths = sorted(glob.glob(gif_images_dir))\n",
        "img_paths = img_paths[:total_timesteps]\n",
        "img_paths = img_paths[::step]\n",
        "\n",
        "\n",
        "print(\"total frames in gif : \", len(img_paths))\n",
        "print(\"total duration of gif : \" + str(round(len(img_paths) * frame_duration / 1000, 2)) + \" seconds\")\n",
        "\n",
        "\n",
        "\n",
        "# save gif\n",
        "img, *imgs = [Image.open(f) for f in img_paths]\n",
        "img.save(fp=gif_path, format='GIF', append_images=imgs, save_all=True, optimize=True, duration=frame_duration, loop=0)\n",
        "\n",
        "print(\"saved gif at : \", gif_path)\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20d1bR8xHK5j"
      },
      "outputs": [],
      "source": [
        "\n",
        "############################# check gif byte size ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "gif_dir = \"PPO_gifs/\" + env_name + '/*.gif'\n",
        "\n",
        "gif_paths = sorted(glob.glob(gif_dir))\n",
        "\n",
        "for gif_path in gif_paths:\n",
        "    file_size = os.path.getsize(gif_path)\n",
        "    print(gif_path + '\\t\\t' + str(round(file_size / (1024 * 1024), 2)) + \" MB\")\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM5UIAkcGxeA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part V ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YUzQOu1HYHR"
      },
      "source": [
        "################################################################################\n",
        "\n",
        "---------------------------------------------------------------------------- That's all folks ! ----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "################################################################################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "e7JowRQEGGKQ",
        "Z4VJcUT2GlJz"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}