{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JowRQEGGKQ"
      },
      "source": [
        "################################################################################\n",
        "> # **Clone GitHub repository**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "IyGzuEMQF6sJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0eaded1b-8b34-4856-ae33-bf826b55dabb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "fatal: destination path 'PPO-PyTorch' already exists and is not an empty directory.\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "################# Clone repository from github to colab session ################\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to clone all the preTrained networks, logs, graph figures, gifs\n",
        "from the GitHub repository to this colab session\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "!git clone https://github.com/nikhilbarhate99/PPO-PyTorch\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrn6rpJpF8Sc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to copy all files and folders from cloned folder (PPO-PyTorch)\n",
        "to current directory (/content/ or ./)\n",
        "\n",
        "So you can load preTrained networks and log files without changing any paths\n",
        "\n",
        "**  This will overwrite any saved networks, logs, graph figures, or gifs\n",
        "    that are created in this session before copying having the same name (or number)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "!cp -rv ./PPO-PyTorch/* ./\n",
        "\n",
        "print(\"============================================================================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-7AbGA2F8Ut",
        "outputId": "c6921fe3-fa71-42df-e3fa-c9af7eee6aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to delete original cloned folder and the cloned ipynb file\n",
        "(after you have copied its contents to current directory)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "# delete original cloned folder\n",
        "!rm -r ./PPO-PyTorch\n",
        "\n",
        "# delete cloned ipynb file\n",
        "!rm ./PPO_colab.ipynb\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4VJcUT2GlJz"
      },
      "source": [
        "################################################################################\n",
        "> # **Install Dependencies**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "rbpSQTflGlAr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f4dc6cfe-8b47-44fd-9c1b-2a4e60d764a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: gymnasium[classic_control] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[classic_control]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[classic_control]) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[classic_control]) (4.13.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[classic_control]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[classic_control]) (2.6.1)\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Downloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.8.0 mujoco-3.3.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "############ install compatible version of OpenAI roboschool and gym ###########\n",
        "\n",
        "!pip install swig\n",
        "\n",
        "!pip install gymnasium[classic_control]\n",
        "\n",
        "!pip install gymnasium[mujoco]\n",
        "# !pip install roboschool==1.0.7 gym==0.15.4\n",
        "\n",
        "# !pip install box2d-py\n",
        "\n",
        "# !pip install Box2D\n",
        "\n",
        "# !pip install pybullet\n",
        "\n",
        "# !pip install gym[box2d]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzZairIiGQ11"
      },
      "source": [
        "################################################################################\n",
        "> # **Introduction**\n",
        "> The notebook is divided into 5 major parts :\n",
        "\n",
        "*   **Part I** : define actor-critic network and PPO algorithm\n",
        "*   **Part II** : train PPO algorithm and save network weights and log files\n",
        "*   **Part III** : load (preTrained) network weights and test PPO algorithm\n",
        "*   **Part IV** : load log files and plot graphs\n",
        "*   **Part V** : install xvbf, load (preTrained) network weights and save images for gif and then generate gif\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s37cJXAYGrTY"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - I**\n",
        "\n",
        "*   define actor critic networks\n",
        "*   define PPO algorithm\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UT6VUBg-F8Zm",
        "outputId": "8ea0396e-91d1-4c5e-951d-0e5ddfe49ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Device set to : cpu\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "############################### Import libraries ###############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import\n",
        "# import roboschool\n",
        "import pybullet_envs\n",
        "\n",
        "\n",
        "################################## set device ##################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# set device to cpu or cuda\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################## PPO Policy ##################################\n",
        "\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.state_values = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.state_values[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_dim = action_dim\n",
        "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
        "\n",
        "        # actor\n",
        "        if has_continuous_action_space :\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Tanh()\n",
        "                        )\n",
        "        else:\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Softmax(dim=-1)\n",
        "                        )\n",
        "\n",
        "\n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 1)\n",
        "                    )\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        state_val = self.critic(state)\n",
        "\n",
        "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
        "\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            action_var = self.action_var.expand_as(action_mean)\n",
        "            cov_mat = torch.diag_embed(action_var).to(device)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "\n",
        "            # for single action continuous environments\n",
        "            if self.action_dim == 1:\n",
        "                action = action.reshape(-1, self.action_dim)\n",
        "\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "\n",
        "        return action_logprobs, state_values, dist_entropy\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_std = action_std_init\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = new_action_std\n",
        "            self.policy.set_action_std(new_action_std)\n",
        "            self.policy_old.set_action_std(new_action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = self.action_std - action_std_decay_rate\n",
        "            self.action_std = round(self.action_std, 4)\n",
        "            if (self.action_std <= min_action_std):\n",
        "                self.action_std = min_action_std\n",
        "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
        "            else:\n",
        "                print(\"setting actor output action_std to : \", self.action_std)\n",
        "            self.set_action_std(self.action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
        "\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "            self.buffer.state_values.append(state_val)\n",
        "\n",
        "            return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "            self.buffer.state_values.append(state_val)\n",
        "\n",
        "            return action.item()\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        # Monte Carlo estimate of returns\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
        "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
        "\n",
        "        # calculate advantages\n",
        "        advantages = rewards.detach() - old_state_values.detach()\n",
        "\n",
        "\n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            # Evaluating old actions and values\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "\n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # final loss of clipped objective PPO\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
        "\n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # clear buffer\n",
        "        self.buffer.clear()\n",
        "\n",
        "\n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "\n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xCb_EyxF8cF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part I ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr-ZjT_CGyEi"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - II**\n",
        "\n",
        "*   train PPO algorithm on environments\n",
        "*   save preTrained networks weights and log files\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "id": "Tb-U9U8TZEGQ",
        "outputId": "89cc2251-cc7f-4920-d9ca-25d7e24bf89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379441 sha256=9068a1ba137d88a17bb9d32fbf486b019870fcfd07437c51cb264725b625e3ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6031
        },
        "id": "YY1-DzVCF8eh",
        "outputId": "dca9c22f-9e48-49fc-e182-57c819925d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "training environment name : LunarLander-v3\n",
            "current logging run number for LunarLander-v3 :  0\n",
            "logging at : PPO_logs/LunarLander-v3//PPO_LunarLander-v3_log_0.csv\n",
            "save checkpoint path : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "max training timesteps :  1000000\n",
            "max timesteps per episode :  300\n",
            "model saving frequency : 50000 timesteps\n",
            "log frequency : 600 timesteps\n",
            "printing average reward over episodes in last : 2400 timesteps\n",
            "--------------------------------------------------------------------------------------------\n",
            "state space dimension :  8\n",
            "action space dimension :  4\n",
            "--------------------------------------------------------------------------------------------\n",
            "Initializing a discrete action space policy\n",
            "--------------------------------------------------------------------------------------------\n",
            "PPO update frequency : 1200 timesteps\n",
            "PPO K epochs :  40\n",
            "PPO epsilon clip :  0.2\n",
            "discount factor (gamma) :  0.99\n",
            "--------------------------------------------------------------------------------------------\n",
            "optimizer learning rate actor :  0.0003\n",
            "optimizer learning rate critic :  0.001\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2025-04-05 00:42:45\n",
            "============================================================================================\n",
            "Episode : 26 \t\t Timestep : 2400 \t\t Average Reward : -168.98\n",
            "Episode : 49 \t\t Timestep : 4800 \t\t Average Reward : -187.88\n",
            "Episode : 73 \t\t Timestep : 7200 \t\t Average Reward : -151.21\n",
            "Episode : 96 \t\t Timestep : 9600 \t\t Average Reward : -148.66\n",
            "Episode : 120 \t\t Timestep : 12000 \t\t Average Reward : -117.76\n",
            "Episode : 143 \t\t Timestep : 14400 \t\t Average Reward : -101.58\n",
            "Episode : 167 \t\t Timestep : 16800 \t\t Average Reward : -108.5\n",
            "Episode : 189 \t\t Timestep : 19200 \t\t Average Reward : -77.62\n",
            "Episode : 210 \t\t Timestep : 21600 \t\t Average Reward : -69.97\n",
            "Episode : 225 \t\t Timestep : 24000 \t\t Average Reward : -17.15\n",
            "Episode : 241 \t\t Timestep : 26400 \t\t Average Reward : -28.94\n",
            "Episode : 255 \t\t Timestep : 28800 \t\t Average Reward : -37.47\n",
            "Episode : 268 \t\t Timestep : 31200 \t\t Average Reward : -2.39\n",
            "Episode : 284 \t\t Timestep : 33600 \t\t Average Reward : -22.59\n",
            "Episode : 304 \t\t Timestep : 36000 \t\t Average Reward : -16.8\n",
            "Episode : 316 \t\t Timestep : 38400 \t\t Average Reward : -0.81\n",
            "Episode : 329 \t\t Timestep : 40800 \t\t Average Reward : -17.56\n",
            "Episode : 342 \t\t Timestep : 43200 \t\t Average Reward : -18.54\n",
            "Episode : 352 \t\t Timestep : 45600 \t\t Average Reward : -73.09\n",
            "Episode : 363 \t\t Timestep : 48000 \t\t Average Reward : 67.73\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:03\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 371 \t\t Timestep : 50400 \t\t Average Reward : -5.74\n",
            "Episode : 380 \t\t Timestep : 52800 \t\t Average Reward : 43.11\n",
            "Episode : 390 \t\t Timestep : 55200 \t\t Average Reward : 48.34\n",
            "Episode : 401 \t\t Timestep : 57600 \t\t Average Reward : 64.09\n",
            "Episode : 412 \t\t Timestep : 60000 \t\t Average Reward : 7.51\n",
            "Episode : 421 \t\t Timestep : 62400 \t\t Average Reward : 51.39\n",
            "Episode : 430 \t\t Timestep : 64800 \t\t Average Reward : 105.24\n",
            "Episode : 440 \t\t Timestep : 67200 \t\t Average Reward : 68.19\n",
            "Episode : 450 \t\t Timestep : 69600 \t\t Average Reward : 12.12\n",
            "Episode : 461 \t\t Timestep : 72000 \t\t Average Reward : 72.9\n",
            "Episode : 474 \t\t Timestep : 74400 \t\t Average Reward : 25.99\n",
            "Episode : 483 \t\t Timestep : 76800 \t\t Average Reward : 64.52\n",
            "Episode : 493 \t\t Timestep : 79200 \t\t Average Reward : 69.12\n",
            "Episode : 507 \t\t Timestep : 81600 \t\t Average Reward : 34.51\n",
            "Episode : 519 \t\t Timestep : 84000 \t\t Average Reward : 69.54\n",
            "Episode : 528 \t\t Timestep : 86400 \t\t Average Reward : 51.42\n",
            "Episode : 538 \t\t Timestep : 88800 \t\t Average Reward : 111.84\n",
            "Episode : 549 \t\t Timestep : 91200 \t\t Average Reward : -2.48\n",
            "Episode : 562 \t\t Timestep : 93600 \t\t Average Reward : 52.7\n",
            "Episode : 572 \t\t Timestep : 96000 \t\t Average Reward : 74.47\n",
            "Episode : 583 \t\t Timestep : 98400 \t\t Average Reward : 78.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:02:04\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 595 \t\t Timestep : 100800 \t\t Average Reward : 37.79\n",
            "Episode : 606 \t\t Timestep : 103200 \t\t Average Reward : 59.98\n",
            "Episode : 614 \t\t Timestep : 105600 \t\t Average Reward : 82.36\n",
            "Episode : 627 \t\t Timestep : 108000 \t\t Average Reward : 90.61\n",
            "Episode : 639 \t\t Timestep : 110400 \t\t Average Reward : 68.95\n",
            "Episode : 650 \t\t Timestep : 112800 \t\t Average Reward : 63.14\n",
            "Episode : 659 \t\t Timestep : 115200 \t\t Average Reward : 100.84\n",
            "Episode : 670 \t\t Timestep : 117600 \t\t Average Reward : 78.0\n",
            "Episode : 680 \t\t Timestep : 120000 \t\t Average Reward : 88.26\n",
            "Episode : 690 \t\t Timestep : 122400 \t\t Average Reward : 102.17\n",
            "Episode : 700 \t\t Timestep : 124800 \t\t Average Reward : 57.45\n",
            "Episode : 709 \t\t Timestep : 127200 \t\t Average Reward : 101.59\n",
            "Episode : 717 \t\t Timestep : 129600 \t\t Average Reward : 23.61\n",
            "Episode : 726 \t\t Timestep : 132000 \t\t Average Reward : 96.69\n",
            "Episode : 735 \t\t Timestep : 134400 \t\t Average Reward : 103.26\n",
            "Episode : 746 \t\t Timestep : 136800 \t\t Average Reward : 64.15\n",
            "Episode : 756 \t\t Timestep : 139200 \t\t Average Reward : 71.47\n",
            "Episode : 764 \t\t Timestep : 141600 \t\t Average Reward : 116.91\n",
            "Episode : 774 \t\t Timestep : 144000 \t\t Average Reward : 94.76\n",
            "Episode : 786 \t\t Timestep : 146400 \t\t Average Reward : 53.11\n",
            "Episode : 795 \t\t Timestep : 148800 \t\t Average Reward : 122.58\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:03:05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 806 \t\t Timestep : 151200 \t\t Average Reward : 87.31\n",
            "Episode : 814 \t\t Timestep : 153600 \t\t Average Reward : 128.84\n",
            "Episode : 824 \t\t Timestep : 156000 \t\t Average Reward : 101.57\n",
            "Episode : 834 \t\t Timestep : 158400 \t\t Average Reward : 91.89\n",
            "Episode : 846 \t\t Timestep : 160800 \t\t Average Reward : 67.16\n",
            "Episode : 855 \t\t Timestep : 163200 \t\t Average Reward : 78.57\n",
            "Episode : 865 \t\t Timestep : 165600 \t\t Average Reward : 125.89\n",
            "Episode : 874 \t\t Timestep : 168000 \t\t Average Reward : 104.24\n",
            "Episode : 883 \t\t Timestep : 170400 \t\t Average Reward : 110.24\n",
            "Episode : 892 \t\t Timestep : 172800 \t\t Average Reward : 94.27\n",
            "Episode : 900 \t\t Timestep : 175200 \t\t Average Reward : 139.98\n",
            "Episode : 909 \t\t Timestep : 177600 \t\t Average Reward : 158.18\n",
            "Episode : 917 \t\t Timestep : 180000 \t\t Average Reward : 102.03\n",
            "Episode : 925 \t\t Timestep : 182400 \t\t Average Reward : 133.3\n",
            "Episode : 933 \t\t Timestep : 184800 \t\t Average Reward : 119.13\n",
            "Episode : 943 \t\t Timestep : 187200 \t\t Average Reward : 76.82\n",
            "Episode : 951 \t\t Timestep : 189600 \t\t Average Reward : 109.95\n",
            "Episode : 960 \t\t Timestep : 192000 \t\t Average Reward : 101.05\n",
            "Episode : 968 \t\t Timestep : 194400 \t\t Average Reward : 145.29\n",
            "Episode : 977 \t\t Timestep : 196800 \t\t Average Reward : 138.94\n",
            "Episode : 985 \t\t Timestep : 199200 \t\t Average Reward : 138.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:04:06\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 994 \t\t Timestep : 201600 \t\t Average Reward : 130.54\n",
            "Episode : 1004 \t\t Timestep : 204000 \t\t Average Reward : 92.96\n",
            "Episode : 1014 \t\t Timestep : 206400 \t\t Average Reward : 99.56\n",
            "Episode : 1023 \t\t Timestep : 208800 \t\t Average Reward : 114.17\n",
            "Episode : 1032 \t\t Timestep : 211200 \t\t Average Reward : 70.7\n",
            "Episode : 1042 \t\t Timestep : 213600 \t\t Average Reward : 63.72\n",
            "Episode : 1050 \t\t Timestep : 216000 \t\t Average Reward : 116.51\n",
            "Episode : 1059 \t\t Timestep : 218400 \t\t Average Reward : 126.19\n",
            "Episode : 1070 \t\t Timestep : 220800 \t\t Average Reward : 77.22\n",
            "Episode : 1078 \t\t Timestep : 223200 \t\t Average Reward : 156.43\n",
            "Episode : 1087 \t\t Timestep : 225600 \t\t Average Reward : 114.24\n",
            "Episode : 1097 \t\t Timestep : 228000 \t\t Average Reward : 88.36\n",
            "Episode : 1107 \t\t Timestep : 230400 \t\t Average Reward : 77.38\n",
            "Episode : 1115 \t\t Timestep : 232800 \t\t Average Reward : 121.48\n",
            "Episode : 1123 \t\t Timestep : 235200 \t\t Average Reward : 137.37\n",
            "Episode : 1134 \t\t Timestep : 237600 \t\t Average Reward : 76.39\n",
            "Episode : 1142 \t\t Timestep : 240000 \t\t Average Reward : 112.82\n",
            "Episode : 1152 \t\t Timestep : 242400 \t\t Average Reward : 79.54\n",
            "Episode : 1161 \t\t Timestep : 244800 \t\t Average Reward : 111.32\n",
            "Episode : 1171 \t\t Timestep : 247200 \t\t Average Reward : 99.42\n",
            "Episode : 1181 \t\t Timestep : 249600 \t\t Average Reward : 82.91\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:05:09\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1191 \t\t Timestep : 252000 \t\t Average Reward : 113.4\n",
            "Episode : 1202 \t\t Timestep : 254400 \t\t Average Reward : 109.25\n",
            "Episode : 1213 \t\t Timestep : 256800 \t\t Average Reward : 72.14\n",
            "Episode : 1222 \t\t Timestep : 259200 \t\t Average Reward : 138.73\n",
            "Episode : 1231 \t\t Timestep : 261600 \t\t Average Reward : 106.79\n",
            "Episode : 1239 \t\t Timestep : 264000 \t\t Average Reward : 156.32\n",
            "Episode : 1248 \t\t Timestep : 266400 \t\t Average Reward : 147.46\n",
            "Episode : 1257 \t\t Timestep : 268800 \t\t Average Reward : 104.37\n",
            "Episode : 1268 \t\t Timestep : 271200 \t\t Average Reward : 98.51\n",
            "Episode : 1276 \t\t Timestep : 273600 \t\t Average Reward : 153.6\n",
            "Episode : 1286 \t\t Timestep : 276000 \t\t Average Reward : 107.42\n",
            "Episode : 1294 \t\t Timestep : 278400 \t\t Average Reward : 146.39\n",
            "Episode : 1303 \t\t Timestep : 280800 \t\t Average Reward : 149.23\n",
            "Episode : 1311 \t\t Timestep : 283200 \t\t Average Reward : 122.94\n",
            "Episode : 1321 \t\t Timestep : 285600 \t\t Average Reward : 126.92\n",
            "Episode : 1330 \t\t Timestep : 288000 \t\t Average Reward : 141.74\n",
            "Episode : 1339 \t\t Timestep : 290400 \t\t Average Reward : 116.29\n",
            "Episode : 1348 \t\t Timestep : 292800 \t\t Average Reward : 113.74\n",
            "Episode : 1357 \t\t Timestep : 295200 \t\t Average Reward : 116.66\n",
            "Episode : 1365 \t\t Timestep : 297600 \t\t Average Reward : 138.32\n",
            "Episode : 1374 \t\t Timestep : 300000 \t\t Average Reward : 111.64\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:06:11\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1384 \t\t Timestep : 302400 \t\t Average Reward : 95.87\n",
            "Episode : 1394 \t\t Timestep : 304800 \t\t Average Reward : 87.85\n",
            "Episode : 1403 \t\t Timestep : 307200 \t\t Average Reward : 88.97\n",
            "Episode : 1413 \t\t Timestep : 309600 \t\t Average Reward : 112.93\n",
            "Episode : 1422 \t\t Timestep : 312000 \t\t Average Reward : 122.55\n",
            "Episode : 1433 \t\t Timestep : 314400 \t\t Average Reward : 76.46\n",
            "Episode : 1442 \t\t Timestep : 316800 \t\t Average Reward : 124.36\n",
            "Episode : 1450 \t\t Timestep : 319200 \t\t Average Reward : 134.73\n",
            "Episode : 1458 \t\t Timestep : 321600 \t\t Average Reward : 129.67\n",
            "Episode : 1468 \t\t Timestep : 324000 \t\t Average Reward : 119.49\n",
            "Episode : 1479 \t\t Timestep : 326400 \t\t Average Reward : 95.98\n",
            "Episode : 1488 \t\t Timestep : 328800 \t\t Average Reward : 134.48\n",
            "Episode : 1497 \t\t Timestep : 331200 \t\t Average Reward : 180.23\n",
            "Episode : 1508 \t\t Timestep : 333600 \t\t Average Reward : 81.33\n",
            "Episode : 1519 \t\t Timestep : 336000 \t\t Average Reward : 67.22\n",
            "Episode : 1530 \t\t Timestep : 338400 \t\t Average Reward : 105.4\n",
            "Episode : 1540 \t\t Timestep : 340800 \t\t Average Reward : 127.09\n",
            "Episode : 1549 \t\t Timestep : 343200 \t\t Average Reward : 120.77\n",
            "Episode : 1557 \t\t Timestep : 345600 \t\t Average Reward : 147.43\n",
            "Episode : 1566 \t\t Timestep : 348000 \t\t Average Reward : 113.86\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:07:14\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1576 \t\t Timestep : 350400 \t\t Average Reward : 117.47\n",
            "Episode : 1584 \t\t Timestep : 352800 \t\t Average Reward : 140.02\n",
            "Episode : 1593 \t\t Timestep : 355200 \t\t Average Reward : 130.23\n",
            "Episode : 1601 \t\t Timestep : 357600 \t\t Average Reward : 153.38\n",
            "Episode : 1609 \t\t Timestep : 360000 \t\t Average Reward : 126.96\n",
            "Episode : 1619 \t\t Timestep : 362400 \t\t Average Reward : 107.27\n",
            "Episode : 1630 \t\t Timestep : 364800 \t\t Average Reward : 89.23\n",
            "Episode : 1638 \t\t Timestep : 367200 \t\t Average Reward : 126.66\n",
            "Episode : 1648 \t\t Timestep : 369600 \t\t Average Reward : 115.47\n",
            "Episode : 1656 \t\t Timestep : 372000 \t\t Average Reward : 113.37\n",
            "Episode : 1666 \t\t Timestep : 374400 \t\t Average Reward : 115.6\n",
            "Episode : 1675 \t\t Timestep : 376800 \t\t Average Reward : 115.31\n",
            "Episode : 1684 \t\t Timestep : 379200 \t\t Average Reward : 116.29\n",
            "Episode : 1694 \t\t Timestep : 381600 \t\t Average Reward : 85.27\n",
            "Episode : 1703 \t\t Timestep : 384000 \t\t Average Reward : 104.65\n",
            "Episode : 1712 \t\t Timestep : 386400 \t\t Average Reward : 132.88\n",
            "Episode : 1721 \t\t Timestep : 388800 \t\t Average Reward : 131.83\n",
            "Episode : 1729 \t\t Timestep : 391200 \t\t Average Reward : 129.72\n",
            "Episode : 1738 \t\t Timestep : 393600 \t\t Average Reward : 88.33\n",
            "Episode : 1748 \t\t Timestep : 396000 \t\t Average Reward : 123.22\n",
            "Episode : 1756 \t\t Timestep : 398400 \t\t Average Reward : 132.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:08:16\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1764 \t\t Timestep : 400800 \t\t Average Reward : 125.29\n",
            "Episode : 1772 \t\t Timestep : 403200 \t\t Average Reward : 122.91\n",
            "Episode : 1780 \t\t Timestep : 405600 \t\t Average Reward : 141.68\n",
            "Episode : 1789 \t\t Timestep : 408000 \t\t Average Reward : 120.5\n",
            "Episode : 1798 \t\t Timestep : 410400 \t\t Average Reward : 84.15\n",
            "Episode : 1809 \t\t Timestep : 412800 \t\t Average Reward : 66.28\n",
            "Episode : 1818 \t\t Timestep : 415200 \t\t Average Reward : 131.77\n",
            "Episode : 1826 \t\t Timestep : 417600 \t\t Average Reward : 141.63\n",
            "Episode : 1836 \t\t Timestep : 420000 \t\t Average Reward : 112.37\n",
            "Episode : 1846 \t\t Timestep : 422400 \t\t Average Reward : 113.51\n",
            "Episode : 1855 \t\t Timestep : 424800 \t\t Average Reward : 131.39\n",
            "Episode : 1866 \t\t Timestep : 427200 \t\t Average Reward : 106.31\n",
            "Episode : 1874 \t\t Timestep : 429600 \t\t Average Reward : 136.07\n",
            "Episode : 1884 \t\t Timestep : 432000 \t\t Average Reward : 112.33\n",
            "Episode : 1892 \t\t Timestep : 434400 \t\t Average Reward : 121.23\n",
            "Episode : 1902 \t\t Timestep : 436800 \t\t Average Reward : 119.87\n",
            "Episode : 1911 \t\t Timestep : 439200 \t\t Average Reward : 84.19\n",
            "Episode : 1921 \t\t Timestep : 441600 \t\t Average Reward : 99.79\n",
            "Episode : 1931 \t\t Timestep : 444000 \t\t Average Reward : 118.13\n",
            "Episode : 1940 \t\t Timestep : 446400 \t\t Average Reward : 121.83\n",
            "Episode : 1948 \t\t Timestep : 448800 \t\t Average Reward : 126.91\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:09:25\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1959 \t\t Timestep : 451200 \t\t Average Reward : 87.97\n",
            "Episode : 1969 \t\t Timestep : 453600 \t\t Average Reward : 84.18\n",
            "Episode : 1978 \t\t Timestep : 456000 \t\t Average Reward : 123.16\n",
            "Episode : 1987 \t\t Timestep : 458400 \t\t Average Reward : 136.37\n",
            "Episode : 1997 \t\t Timestep : 460800 \t\t Average Reward : 113.25\n",
            "Episode : 2007 \t\t Timestep : 463200 \t\t Average Reward : 91.37\n",
            "Episode : 2018 \t\t Timestep : 465600 \t\t Average Reward : 127.18\n",
            "Episode : 2031 \t\t Timestep : 468000 \t\t Average Reward : 68.48\n",
            "Episode : 2042 \t\t Timestep : 470400 \t\t Average Reward : 111.44\n",
            "Episode : 2053 \t\t Timestep : 472800 \t\t Average Reward : 77.62\n",
            "Episode : 2062 \t\t Timestep : 475200 \t\t Average Reward : 142.95\n",
            "Episode : 2071 \t\t Timestep : 477600 \t\t Average Reward : 126.23\n",
            "Episode : 2082 \t\t Timestep : 480000 \t\t Average Reward : 100.14\n",
            "Episode : 2092 \t\t Timestep : 482400 \t\t Average Reward : 98.56\n",
            "Episode : 2103 \t\t Timestep : 484800 \t\t Average Reward : 102.11\n",
            "Episode : 2112 \t\t Timestep : 487200 \t\t Average Reward : 129.46\n",
            "Episode : 2121 \t\t Timestep : 489600 \t\t Average Reward : 127.5\n",
            "Episode : 2131 \t\t Timestep : 492000 \t\t Average Reward : 124.81\n",
            "Episode : 2140 \t\t Timestep : 494400 \t\t Average Reward : 141.95\n",
            "Episode : 2150 \t\t Timestep : 496800 \t\t Average Reward : 95.5\n",
            "Episode : 2161 \t\t Timestep : 499200 \t\t Average Reward : 120.84\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:10:26\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2171 \t\t Timestep : 501600 \t\t Average Reward : 93.4\n",
            "Episode : 2180 \t\t Timestep : 504000 \t\t Average Reward : 135.98\n",
            "Episode : 2188 \t\t Timestep : 506400 \t\t Average Reward : 116.96\n",
            "Episode : 2200 \t\t Timestep : 508800 \t\t Average Reward : 70.91\n",
            "Episode : 2209 \t\t Timestep : 511200 \t\t Average Reward : 116.29\n",
            "Episode : 2217 \t\t Timestep : 513600 \t\t Average Reward : 135.96\n",
            "Episode : 2225 \t\t Timestep : 516000 \t\t Average Reward : 105.38\n",
            "Episode : 2235 \t\t Timestep : 518400 \t\t Average Reward : 108.16\n",
            "Episode : 2243 \t\t Timestep : 520800 \t\t Average Reward : 143.14\n",
            "Episode : 2252 \t\t Timestep : 523200 \t\t Average Reward : 155.73\n",
            "Episode : 2260 \t\t Timestep : 525600 \t\t Average Reward : 127.06\n",
            "Episode : 2269 \t\t Timestep : 528000 \t\t Average Reward : 139.09\n",
            "Episode : 2279 \t\t Timestep : 530400 \t\t Average Reward : 112.44\n",
            "Episode : 2288 \t\t Timestep : 532800 \t\t Average Reward : 121.98\n",
            "Episode : 2299 \t\t Timestep : 535200 \t\t Average Reward : 91.61\n",
            "Episode : 2309 \t\t Timestep : 537600 \t\t Average Reward : 198.49\n",
            "Episode : 2321 \t\t Timestep : 540000 \t\t Average Reward : 195.47\n",
            "Episode : 2331 \t\t Timestep : 542400 \t\t Average Reward : 118.54\n",
            "Episode : 2340 \t\t Timestep : 544800 \t\t Average Reward : 126.99\n",
            "Episode : 2350 \t\t Timestep : 547200 \t\t Average Reward : 118.39\n",
            "Episode : 2359 \t\t Timestep : 549600 \t\t Average Reward : 93.45\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:11:29\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2367 \t\t Timestep : 552000 \t\t Average Reward : 112.47\n",
            "Episode : 2376 \t\t Timestep : 554400 \t\t Average Reward : 98.87\n",
            "Episode : 2386 \t\t Timestep : 556800 \t\t Average Reward : 83.85\n",
            "Episode : 2395 \t\t Timestep : 559200 \t\t Average Reward : 138.93\n",
            "Episode : 2405 \t\t Timestep : 561600 \t\t Average Reward : 127.82\n",
            "Episode : 2417 \t\t Timestep : 564000 \t\t Average Reward : 148.4\n",
            "Episode : 2427 \t\t Timestep : 566400 \t\t Average Reward : 148.28\n",
            "Episode : 2440 \t\t Timestep : 568800 \t\t Average Reward : 108.51\n",
            "Episode : 2451 \t\t Timestep : 571200 \t\t Average Reward : 51.61\n",
            "Episode : 2463 \t\t Timestep : 573600 \t\t Average Reward : 82.06\n",
            "Episode : 2473 \t\t Timestep : 576000 \t\t Average Reward : 84.36\n",
            "Episode : 2483 \t\t Timestep : 578400 \t\t Average Reward : 125.14\n",
            "Episode : 2495 \t\t Timestep : 580800 \t\t Average Reward : 127.01\n",
            "Episode : 2506 \t\t Timestep : 583200 \t\t Average Reward : 101.68\n",
            "Episode : 2515 \t\t Timestep : 585600 \t\t Average Reward : 65.79\n",
            "Episode : 2529 \t\t Timestep : 588000 \t\t Average Reward : 47.72\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-d4d4b21f7613>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# select action with policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-54207e73b47a>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_logprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-54207e73b47a>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0maction_logprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mstate_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_pmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################################### Training ###################################\n",
        "\n",
        "\n",
        "####### initialize environment hyperparameters ######\n",
        "\n",
        "# 1.\n",
        "# env_name = \"CartPole-v1\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 400                    # max timesteps in one episode\n",
        "# max_training_timesteps = int(2e6)  # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "# print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
        "# log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
        "# save_model_freq = int(1e5)      # save model frequency (in num timesteps)\n",
        "\n",
        "\n",
        "#2.\n",
        "# env_name = \"HalfCheetah-v4\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000                   # max timesteps in one episode\n",
        "# max_training_timesteps = int(3e6)   # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "# print_freq = max_ep_len * 10               # print avg reward in the interval (in num timesteps)\n",
        "# log_freq = max_ep_len * 2                 # log avg reward in the interval (in num timesteps)\n",
        "# save_model_freq = int(1e5)      # save model frequency (in num timesteps)\n",
        "\n",
        "# action_std = 0.6                    # starting std for action distribution (Multivariate Normal)\n",
        "# action_std_decay_rate = 0.05        # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
        "# min_action_std = 0.1                # minimum action_std (stop decay after action_std <= min_action_std)\n",
        "# action_std_decay_freq = int(2.5e5)  # action_std decay frequency (in num timesteps)\n",
        "\n",
        "\n",
        "#3.\n",
        "env_name = \"LunarLander-v3\"\n",
        "has_continuous_action_space = False\n",
        "\n",
        "max_ep_len = 300                   # max timesteps in one episode\n",
        "max_training_timesteps = int(1e6)   # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "print_freq = max_ep_len * 8                # print avg reward in the interval (in num timesteps)\n",
        "log_freq = max_ep_len * 2                  # log avg reward in the interval (in num timesteps)\n",
        "save_model_freq = int(5e4)      # save model frequency (in num timesteps)\n",
        "\n",
        "action_std = None\n",
        "\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "## Note : print/log frequencies should be > than max_ep_len\n",
        "\n",
        "\n",
        "################ PPO hyperparameters ################\n",
        "\n",
        "\n",
        "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
        "K_epochs = 40               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003       # learning rate for actor network\n",
        "lr_critic = 0.001       # learning rate for critic network\n",
        "\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "\n",
        "print(\"training environment name : \" + env_name)\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "###################### logging ######################\n",
        "\n",
        "#### log files for multiple runs are NOT overwritten\n",
        "\n",
        "log_dir = \"PPO_logs\"\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "log_dir = log_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "#### get number of log files in log directory\n",
        "run_num = 0\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "run_num = len(current_num_files)\n",
        "\n",
        "\n",
        "#### create new log file for each run\n",
        "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "\n",
        "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
        "print(\"logging at : \" + log_f_name)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "################### checkpointing ###################\n",
        "\n",
        "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
        "\n",
        "directory = \"PPO_preTrained\"\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "directory = directory + '/' + env_name + '/'\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"save checkpoint path : \" + checkpoint_path)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "############# print all hyperparameters #############\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"max training timesteps : \", max_training_timesteps)\n",
        "print(\"max timesteps per episode : \", max_ep_len)\n",
        "\n",
        "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
        "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
        "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"state space dimension : \", state_dim)\n",
        "print(\"action space dimension : \", action_dim)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "if has_continuous_action_space:\n",
        "    print(\"Initializing a continuous action space policy\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"starting std of action distribution : \", action_std)\n",
        "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
        "    print(\"minimum std of action distribution : \", min_action_std)\n",
        "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
        "\n",
        "else:\n",
        "    print(\"Initializing a discrete action space policy\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
        "print(\"PPO K epochs : \", K_epochs)\n",
        "print(\"PPO epsilon clip : \", eps_clip)\n",
        "print(\"discount factor (gamma) : \", gamma)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"optimizer learning rate actor : \", lr_actor)\n",
        "print(\"optimizer learning rate critic : \", lr_critic)\n",
        "\n",
        "if random_seed:\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"setting random seed to \", random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "################# training procedure ################\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# track total training time\n",
        "start_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# logging file\n",
        "log_f = open(log_f_name,\"w+\")\n",
        "log_f.write('episode,timestep,reward\\n')\n",
        "\n",
        "\n",
        "# printing and logging variables\n",
        "print_running_reward = 0\n",
        "print_running_episodes = 0\n",
        "\n",
        "log_running_reward = 0\n",
        "log_running_episodes = 0\n",
        "\n",
        "time_step = 0\n",
        "i_episode = 0\n",
        "\n",
        "\n",
        "# training loop\n",
        "while time_step <= max_training_timesteps:\n",
        "\n",
        "    state,_ = env.reset()\n",
        "    current_ep_reward = 0\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "\n",
        "        # select action with policy\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # saving reward and is_terminals\n",
        "        ppo_agent.buffer.rewards.append(reward)\n",
        "        ppo_agent.buffer.is_terminals.append(done)\n",
        "\n",
        "        time_step +=1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        # update PPO agent\n",
        "        if time_step % update_timestep == 0:\n",
        "            ppo_agent.update()\n",
        "\n",
        "        # if continuous action space; then decay action std of ouput action distribution\n",
        "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
        "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
        "\n",
        "        # log in logging file\n",
        "        if time_step % log_freq == 0:\n",
        "\n",
        "            # log average reward till last episode\n",
        "            log_avg_reward = log_running_reward / log_running_episodes\n",
        "            log_avg_reward = round(log_avg_reward, 4)\n",
        "\n",
        "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
        "            log_f.flush()\n",
        "\n",
        "            log_running_reward = 0\n",
        "            log_running_episodes = 0\n",
        "\n",
        "        # printing average reward\n",
        "        if time_step % print_freq == 0:\n",
        "\n",
        "            # print average reward till last episode\n",
        "            print_avg_reward = print_running_reward / print_running_episodes\n",
        "            print_avg_reward = round(print_avg_reward, 2)\n",
        "\n",
        "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
        "\n",
        "            print_running_reward = 0\n",
        "            print_running_episodes = 0\n",
        "\n",
        "        # save model weights\n",
        "        if time_step % save_model_freq == 0:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"saving model at : \" + checkpoint_path)\n",
        "            ppo_agent.save(checkpoint_path)\n",
        "            print(\"model saved\")\n",
        "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        # break; if the episode is over\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print_running_reward += current_ep_reward\n",
        "    print_running_episodes += 1\n",
        "\n",
        "    log_running_reward += current_ep_reward\n",
        "    log_running_episodes += 1\n",
        "\n",
        "    i_episode += 1\n",
        "\n",
        "\n",
        "log_f.close()\n",
        "env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print total training time\n",
        "print(\"============================================================================================\")\n",
        "end_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "print(\"Finished training at (GMT) : \", end_time)\n",
        "print(\"Total training time  : \", end_time - start_time)\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "UEy2qKdZF8ha"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part II ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhK13_1G6zX"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - III**\n",
        "\n",
        "*   load and test preTrained networks on environments\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SZWyhkq9Gxm5",
        "outputId": "c95cf703-b852-4781-a3f2-5205764c5f02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading network from : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode: 1 \t\t Reward: 301.19\n",
            "Episode: 2 \t\t Reward: 223.65\n",
            "Episode: 3 \t\t Reward: 158.73\n",
            "Episode: 4 \t\t Reward: 225.02\n",
            "Episode: 5 \t\t Reward: 196.07\n",
            "Episode: 6 \t\t Reward: 146.23\n",
            "Episode: 7 \t\t Reward: 132.49\n",
            "Episode: 8 \t\t Reward: 239.57\n",
            "Episode: 9 \t\t Reward: 278.65\n",
            "Episode: 10 \t\t Reward: 101.48\n",
            "============================================================================================\n",
            "average test reward : 200.31\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "#################################### Testing ###################################\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "# env_name = \"CartPole-v1\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 400\n",
        "# action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"HalfCheetah-v4\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000\n",
        "# action_std = 0.6\n",
        "\n",
        "env_name = \"LunarLander-v3\"\n",
        "has_continuous_action_space = False\n",
        "max_ep_len = 300\n",
        "action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "# env_name = \"RoboschoolWalker2d-v1\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 10    # total num of testing episodes\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003           # learning rate for actor\n",
        "lr_critic = 0.001           # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "    ep_reward = 0\n",
        "    state,_ = env.reset()\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # clear buffer\n",
        "    ppo_agent.buffer.clear()\n",
        "\n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "n6IYC_JCGxlB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part III ###############################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZewQELovHFt4"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - IV**\n",
        "\n",
        "*   load log files using pandas\n",
        "*   plot graph using matplotlib\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "bY-E5HGcGxiu",
        "outputId": "4338f867-fecf-4b1d-d7b8-12a5de4a967e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading data from : PPO_logs/LunarLander-v3//PPO_LunarLander-v3_log_0.csv\n",
            "data shape :  (980, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "figure saved at :  PPO_figs/LunarLander-v3//PPO_LunarLander-v3_fig_0ECE590.png\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "# env_name = 'CartPole-v1'\n",
        "#env_name = \"HalfCheetah-v4\"\n",
        "env_name = 'LunarLander-v3'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "fig_num = 0     #### change this to prevent overwriting figures in same env_name folder\n",
        "\n",
        "plot_avg = True    # plot average of all runs; else plot all runs separately\n",
        "\n",
        "fig_width = 10\n",
        "fig_height = 6\n",
        "\n",
        "\n",
        "# smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "window_len_smooth = 50\n",
        "min_window_len_smooth = 1\n",
        "linewidth_smooth = 1.5\n",
        "alpha_smooth = 1\n",
        "\n",
        "window_len_var = 5\n",
        "min_window_len_var = 1\n",
        "linewidth_var = 2\n",
        "alpha_var = 0.1\n",
        "\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'olive', 'brown', 'magenta', 'cyan', 'crimson','gray', 'black']\n",
        "\n",
        "\n",
        "# make directory for saving figures\n",
        "figures_dir = \"PPO_figs\"\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "# make environment directory for saving figures\n",
        "figures_dir = figures_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "\n",
        "fig_save_path = figures_dir + '/PPO_' + env_name + '_fig_' + str(fig_num) + \"ECE590\" + '.png'\n",
        "\n",
        "\n",
        "# get number of log files in directory\n",
        "log_dir = \"PPO_logs\" + '/' + env_name + '/'\n",
        "\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "num_runs = len(current_num_files)\n",
        "\n",
        "\n",
        "all_runs = []\n",
        "\n",
        "for run_num in range(num_runs):\n",
        "\n",
        "    log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "    print(\"loading data from : \" + log_f_name)\n",
        "    data = pd.read_csv(log_f_name)\n",
        "    data = pd.DataFrame(data)\n",
        "\n",
        "    print(\"data shape : \", data.shape)\n",
        "\n",
        "    all_runs.append(data)\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "if plot_avg:\n",
        "    # average all runs\n",
        "    df_concat = pd.concat(all_runs)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "    data_avg['reward_smooth'] = data_avg['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "    data_avg['reward_var'] = data_avg['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_smooth',ax=ax,color=colors[0],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_var',ax=ax,color=colors[0],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep only reward_smooth in the legend and rename it\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
        "\n",
        "\n",
        "else:\n",
        "    for i, run in enumerate(all_runs):\n",
        "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "        run['reward_smooth_' + str(i)] = run['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "        run['reward_var_' + str(i)] = run['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "        # plot the lines\n",
        "        run.plot(kind='line', x='timestep' , y='reward_smooth_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "        run.plot(kind='line', x='timestep' , y='reward_var_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep alternate elements (reward_smooth_i) in the legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_handles = []\n",
        "    new_labels = []\n",
        "    for i in range(len(handles)):\n",
        "        if(i%2 == 0):\n",
        "            new_handles.append(handles[i])\n",
        "            new_labels.append(labels[i])\n",
        "    ax.legend(new_handles, new_labels, loc=2)\n",
        "\n",
        "\n",
        "\n",
        "# ax.set_yticks(np.arange(0, 1800, 200))\n",
        "# ax.set_xticks(np.arange(0, int(4e6), int(5e5)))\n",
        "\n",
        "\n",
        "ax.grid(color='gray', linestyle='-', linewidth=1, alpha=0.2)\n",
        "\n",
        "ax.set_xlabel(\"Timesteps\", fontsize=12)\n",
        "ax.set_ylabel(\"Rewards\", fontsize=12)\n",
        "\n",
        "plt.title(env_name, fontsize=14)\n",
        "\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(fig_width, fig_height)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "plt.savefig(fig_save_path)\n",
        "print(\"figure saved at : \", fig_save_path)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "YaWPRW9EGxgH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part IV ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8uG43MtHNGC"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - V**\n",
        "\n",
        "*   install virtual display libraries for rendering on colab / remote server ^\n",
        "*   load preTrained networks and save images for gif\n",
        "*   generate and save gif from previously saved images\n",
        "\n",
        "*   ^ If running locally; do not install xvbf and pyvirtualdisplay. Just comment out the virtual display code and render it normally.\n",
        "*   ^ You will still require to use ipythondisplay, if you want to render it in the Jupyter Notebook.\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "VL3tpKf3HLAq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#### to render on colab / server / headless machine install virtual display libraries\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y xvfb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RLhLOO713mfa",
        "outputId": "3dbe45d6-5425-48f2-be1c-45c2502deeb8"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [1 InRelease 14.2 kB/129 kB 11%] [Connected t\r                                                                                                    \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 384 kB in 6s (62.8 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.14).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "j5Rx_IFKHK-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7e3b6a5c-3993-409f-d3d2-1d05ad12d9b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading network from : PPO_preTrained/LunarLander-v3/PPO_LunarLander-v3_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode: 1 \t\t Reward: 121.68\n",
            "============================================================================================\n",
            "total number of frames / timesteps / images saved :  300\n",
            "average test reward : 121.68\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "############################# save images for gif ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import gymnasium as gym\n",
        "#import roboschool\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "One frame corresponding to each timestep is saved in a folder :\n",
        "\n",
        "PPO_gif_images/env_name/000001.jpg\n",
        "PPO_gif_images/env_name/000002.jpg\n",
        "PPO_gif_images/env_name/000003.jpg\n",
        "...\n",
        "...\n",
        "...\n",
        "\n",
        "\n",
        "if this section is run multiple times or for multiple episodes for the same env_name;\n",
        "then the saved images will be overwritten.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### beginning of virtual display code section\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "#### end of virtual display code section\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "# env_name = \"CartPole-v1\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 400\n",
        "# action_std = None\n",
        "\n",
        "# env_name = \"HalfCheetah-v4\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 400\n",
        "# action_std = 0.6\n",
        "\n",
        "env_name = \"LunarLander-v3\"\n",
        "has_continuous_action_space = False\n",
        "max_ep_len = 300\n",
        "action_std = None\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "# env_name = \"RoboschoolWalker2d-v1\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 1     # save gif for only one episode\n",
        "\n",
        "render_ipython = False      # plot the images using matplotlib and ipythondisplay before saving (slow)\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003         # learning rate for actor\n",
        "lr_critic = 0.001         # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name,render_mode=\"rgb_array\")\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "# make directory for saving gif images\n",
        "gif_images_dir = \"PPO_gif_images\" + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make environment directory for saving gif images\n",
        "gif_images_dir = gif_images_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make directory for gif\n",
        "gif_dir = \"PPO_gifs\" + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "# make environment directory for gif\n",
        "gif_dir = gif_dir + '/' + env_name  + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "\n",
        "\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "\n",
        "    ep_reward = 0\n",
        "    state,_ = env.reset()\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _, _= env.step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        img = env.render()\n",
        "\n",
        "\n",
        "        #### beginning of ipythondisplay code section 1\n",
        "\n",
        "        if render_ipython:\n",
        "            plt.imshow(img)\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "\n",
        "        #### end of ipythondisplay code section 1\n",
        "\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(gif_images_dir + '/' + str(t).zfill(6) + '.jpg')\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # clear buffer\n",
        "    ppo_agent.buffer.clear()\n",
        "\n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "#### beginning of ipythondisplay code section 2\n",
        "\n",
        "if render_ipython:\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "\n",
        "#### end of ipythondisplay code section 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "print(\"total number of frames / timesteps / images saved : \", t)\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "BoVshl_ZHK7s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6d34bdf1-6c13-4f22-efb9-509b3f7359cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "total frames in gif :  30\n",
            "total duration of gif : 4.5 seconds\n",
            "saved gif at :  PPO_gifs/LunarLander-v3/PPO_LunarLander-v3_gif_0ECE590.gif\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "######################## generate gif from saved images ########################\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "#env_name = 'CartPole-v1'\n",
        "# env_name = \"HalfCheetah-v4\"\n",
        "env_name = 'LunarLander-v3'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "gif_num = 0     #### change this to prevent overwriting gifs in same env_name folder\n",
        "\n",
        "# adjust following parameters to get desired duration, size (bytes) and smoothness of gif\n",
        "total_timesteps = 300\n",
        "step = 10\n",
        "frame_duration = 150\n",
        "\n",
        "\n",
        "# input images\n",
        "gif_images_dir = \"PPO_gif_images/\" + env_name + '/*.jpg'\n",
        "\n",
        "\n",
        "# ouput gif path\n",
        "gif_dir = \"PPO_gifs\"\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_dir = gif_dir + '/' + env_name\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_path = gif_dir + '/PPO_' + env_name + '_gif_' + str(gif_num) + \"ECE590\"+ '.gif'\n",
        "\n",
        "\n",
        "\n",
        "img_paths = sorted(glob.glob(gif_images_dir))\n",
        "img_paths = img_paths[:total_timesteps]\n",
        "img_paths = img_paths[::step]\n",
        "\n",
        "\n",
        "print(\"total frames in gif : \", len(img_paths))\n",
        "print(\"total duration of gif : \" + str(round(len(img_paths) * frame_duration / 1000, 2)) + \" seconds\")\n",
        "\n",
        "\n",
        "\n",
        "# save gif\n",
        "img, *imgs = [Image.open(f) for f in img_paths]\n",
        "img.save(fp=gif_path, format='GIF', append_images=imgs, save_all=True, optimize=True, duration=frame_duration, loop=0)\n",
        "\n",
        "print(\"saved gif at : \", gif_path)\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "20d1bR8xHK5j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4b7e2e91-22ef-4d5f-ba65-dd07f6a69ec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "PPO_gifs/LunarLander-v3/PPO_LunarLander-v3_gif_0ECE590.gif\t\t0.22 MB\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "############################# check gif byte size ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "# env_name = 'CartPole-v1'\n",
        "# env_name = \"HalfCheetah-v4\"\n",
        "env_name = 'LunarLander-v3'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "gif_dir = \"PPO_gifs/\" + env_name + '/*.gif'\n",
        "\n",
        "gif_paths = sorted(glob.glob(gif_dir))\n",
        "\n",
        "for gif_path in gif_paths:\n",
        "    file_size = os.path.getsize(gif_path)\n",
        "    print(gif_path + '\\t\\t' + str(round(file_size / (1024 * 1024), 2)) + \" MB\")\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM5UIAkcGxeA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part V ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YUzQOu1HYHR"
      },
      "source": [
        "################################################################################\n",
        "\n",
        "---------------------------------------------------------------------------- That's all folks ! ----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "################################################################################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "e7JowRQEGGKQ",
        "Z4VJcUT2GlJz"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}